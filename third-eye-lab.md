# Third Eye Lab  
**Role:** UX Researcher & Prototype Developer  
**Penn State University** | **2 Years**  
_Assistive Technology, Haptics, C# Prototyping_

---

## Overview  
As part of the NSF-funded *Visual Cortex on Silicon* program, I contributed to the development of a wearable system to support blind and low vision individuals in navigating grocery stores independently. Our prototype used a palm-mounted camera and real-time feedback to guide users to specific products. I contributed to **user research, experimental testing**, and **hardware-software prototyping**, including modifications to the device’s C# codebase.

**Watch the project featured on BTN LiveBIG:**  
[Third Eye on YouTube](https://www.youtube.com/watch?v=kz_Tbtz9T98)

![Wearable glove prototype with haptic feedback](https://npr.brightspotcdn.com/dims4/default/b8bfb72/2147483647/strip/true/crop/333x265+0+0/resize/840x668!/format/webp/quality/90/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Flegacy%2Fsites%2Fwpsu%2Ffiles%2F201512%2Fscreen_shot_2015-12-09_at_8.05.30_pm.png "Third Eye wearable glove prototype")


---

## Problem  
Visually impaired individuals face significant challenges in everyday shopping:

- Locating specific products without sight  
- Navigating crowded or unfamiliar store layouts  
- Interpreting visual-only information like shelf labels  

Traditional assistive technologies often rely on screens or voice commands and are difficult to use in real-world, physical environments where visual access is limited or impossible.

---

## Process  

### User Research & Interview Analysis  
- Conducted qualitative interviews with blind and low vision participants to understand their current strategies and frustrations while shopping.  
- Transcribed and analyzed interview data to identify key usability themes, including the difficulty of locating items, interpreting product differences, and depending on external assistance.

### Experimental Testing  
- Helped facilitate a structured set of **in-lab user trials** where participants navigated a **mock grocery store layout** while wearing our prototype glove.  
- Observed and recorded how participants responded to directional feedback, evaluated their success in locating items, and gathered feedback on comfort, clarity, and reliability of guidance.

### C# Prototyping & Feedback Design  
- Modified the glove's embedded logic using C# to explore different feedback styles, such as switching from vibration to **audio feedback**.  
- Iterated on the **frequency and patterning** of signals to determine what participants found easiest to interpret for directional guidance.  
- Focused on making feedback more distinct and less ambiguous, especially when users needed to make quick decisions.

---

## Outcome  
- Participants successfully used the prototype to locate specific products with increased independence and reduced reliance on verbal instruction.  
- Experiments showed the system’s potential to provide effective real-time guidance using a simple, wearable device.  
- The project received public attention through media features and peer-reviewed research contributions.

**Read the published research:**  
[Third Eye: A Shopping Assistant for the Visually Impaired – IEEE Computer, 2017](https://sooyeon-lee.github.io/files/2017_third.pdf)

**Project spotlight from Penn State:**  
[Seeing Without Sight (Penn State Impact)](https://www.psu.edu/impact/story/seeing-without-sight/)

---

## Key Takeaways  
- Iterative testing with real users led to actionable insights on how to design non-visual feedback that is both intuitive and usable.  
- Seemingly small changes in how signals are delivered (timing, format, modality) can make a significant difference in usability.  
- Direct collaboration with the visually impaired community was essential to ensuring the system addressed real-world needs—not just technical goals.